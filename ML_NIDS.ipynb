{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0129af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyshark\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ccef6",
   "metadata": {},
   "source": [
    "The files necessary for download can be found at: https://www.kaggle.com/datasets/cicdataset/cicids2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89441553",
   "metadata": {},
   "source": [
    "## User Input Cell for data aggregation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter whether file formats for aggregation are PCAP or CSV\n",
    "PCAP_OR_CSV = 'CSV'\n",
    "#Give absolute file path where all file(s) are stored\n",
    "file_path = r'C:\\Users\\17272\\Desktop\\ML_Datasets'\n",
    "#if your file has a name you'd like to refrence in the data set and know where it can be parsed from the in file path, \n",
    "#give the position of the file name\n",
    "split_file_name = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0f839",
   "metadata": {},
   "source": [
    "## Function for Aggregating Data in CSV or PCAP and outputing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ca5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcap_or_csv_to_pandas(file_type,file_path,str_split = 'N/A'):\n",
    "    #If user wants to keep the file name that the data originated from\n",
    "    if file_type == 'CSV' and str_split != 'N/A':\n",
    "        absolute_file_path = file_path + '\\\\*.csv'\n",
    "        file_list = glob.glob(absolute_file_path)\n",
    "        PCAP_DF = pd.DataFrame()\n",
    "        for file in file_list:\n",
    "            file_name = file.split('\\\\')[str_split]\n",
    "            temp_df = pd.read_csv(file)\n",
    "            temp_df['File_Name'] = file_name\n",
    "            PCAP_DF = pd.concat([PCAP_DF,temp_df], ignore_index = True)\n",
    "        PCAP_DF.columns = PCAP_DF.columns.str.strip()\n",
    "        return PCAP_DF\n",
    "    #Name of file omitted from DF\n",
    "    elif file_type == 'CSV' and str_split == 'N/A':\n",
    "        absolute_file_path = file_path + '\\\\*.csv'\n",
    "        file_list = glob.glob(absolute_file_path)\n",
    "        PCAP_DF = pd.DataFrame()\n",
    "        for file in file_list:\n",
    "            temp_df = pd.read_csv(file)\n",
    "            PCAP_DF = pd.concat([PCAP_DF,temp_df], ignore_index = True)\n",
    "        PCAP_DF.columns = PCAP_DF.columns.str.strip()\n",
    "        return PCAP_DF\n",
    "    #If files need to be aggregated from PCAP files instead of CSV\n",
    "    elif file_type == 'PCAP':\n",
    "        absolute_file_path = file_path + '\\\\*.pcap'\n",
    "        pcap_list = glob.glob(absolute_file_path)\n",
    "        for pcap in pcap_list:\n",
    "            cap = pyshark.FileCapture(pcap)\n",
    "            data = []\n",
    "            for packet in cap:\n",
    "                packet_info = {}\n",
    "                # Extract desired information from each packet\n",
    "                try:\n",
    "                    packet_info['timestamp'] = packet.sniff_timestamp\n",
    "                    packet_info['source_ip'] = packet.ip.src\n",
    "                    packet_info['destination_ip'] = packet.ip.dst\n",
    "                    packet_info['protocol'] = packet.transport_layer\n",
    "                    packet_info['length'] = packet.length\n",
    "                    # Add other fields as needed\n",
    "                except AttributeError:\n",
    "                    # Skip if an attribute is not found in the packet\n",
    "                    continue\n",
    "                data.append(packet_info)\n",
    "                cap.close()\n",
    "            return pd.DataFrame(data)\n",
    "    else:\n",
    "        print(\"Please enter valid file type: CSV/PCAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d97d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    # Select columns that are either float or int\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Check for NaN and infinite values in these columns\n",
    "    mask = df[numeric_cols].applymap(lambda x: np.isnan(x) or np.isinf(x))\n",
    "    \n",
    "    # Drop rows where any of these columns have NaN or infinite values\n",
    "    df_cleaned = df[~mask.any(axis=1)]\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812a73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Call for aggregating files\n",
    "PCAP_DF = pcap_or_csv_to_pandas(PCAP_OR_CSV,file_path)\n",
    "#Function call for removing rows with NaN or infinite values\n",
    "PCAP_DF_Cleaned = clean_dataset(PCAP_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f34b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat a copy of dataset to ensure integirty of original dataset is kept\n",
    "PCAP_DF_Simplifed = PCAP_DF_Cleaned.copy()\n",
    "#Simplifying dataset to either indicate a row item is benign or malicious\n",
    "PCAP_DF_Simplifed['BENIGN/ATTACK'] = np.where(PCAP_DF_Simplifed['Label'] != 'BENIGN', 'ATTACK', 'BENIGN')\n",
    "#dropping Label column from dataset since BENIGN/ATTACK column was created\n",
    "PCAP_DF_Simplifed = PCAP_DF_Simplifed.drop(columns = ['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5728c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input dataset for training by dropping 'BENIGN/ATTACK'\n",
    "#X = PCAP_DF_Simplifed.drop(columns = ['BENIGN/ATTACK'])\n",
    "\n",
    "#Values/output from dataset is 'BENIGN/ATTACK'\n",
    "#y = PCAP_DF_Simplifed['BENIGN/ATTACK']\n",
    "\n",
    "#splitting datset into 2, 80% is to train the model and 20% is to test the predictions of the model\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#Using a decision tree machine learning model and fitting it using the X and y training sets\n",
    "#model = DecisionTreeClassifier()\n",
    "#model.fit(X_train,y_train)\n",
    "\n",
    "#Testing the predictions of the model\n",
    "#predictions = model.predict(X_test)\n",
    "#score = accuracy_score(y_test, predictions)\n",
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774ff57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After training the model I export the it so I don't have to continually train it and can just load it to make predictions\n",
    "#joblib.dump(model, 'CYBER-ATTACK_PREDICTIONS.joblib')\n",
    "\n",
    "#This loads the exported model above, I can continue to aggregate datasets and make predictions based on the trained model \n",
    "#as long as all columns of dataset loaded are the same \n",
    "#model = joblib.load('CYBER-ATTACK_PREDICTIONS.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25615b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
